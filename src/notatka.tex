\documentclass{article}
\setlength{\headheight}{8pt}

\usepackage[a4paper, margin=0.3in]{geometry}
\usepackage{lmodern}
\usepackage[polish]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{hyperref}
\usepackage[shortlabels]{enumitem}
\usepackage{physics}
\usepackage{multicol}

% Definicje środowisk do twierdzeń, lematów, dowodów itp.
\newtheorem{theorem}{Twierdzenie}[section]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\theoremstyle{definition}
\newtheorem{definition}{Definicja}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Uwaga}
\newtheorem*{example}{Przykład}
\newtheorem*{property}{Własność}

\title{Probability Test Notes}
\author{Marcin Szopa}
\date{03.12.2024}

% Remove padding at the top of subsections
\usepackage{titlesec}
\titlespacing*{\subsection}{0pt}{*0}{*0}

\begin{document}

\begin{multicols}{2}
    \section*{Markow}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item Macierz przejścia: \( P = [p_{ij}] \), gdzie \( p_{ij} = P(X_{n+1} = j \mid X_n = i) \).
        \item Wiersze macierzy sumują się do 1.
        \item Rozkład \(X_t\) reprezentowany przez wektor \(\pi(t)\): \(\pi(t)_s = P(X_t = s)\). Wtedy \(\pi(t + 1) = \pi(t)M\) i ogólniej \(\pi(t + s) = \pi(t)M^s\).
        \item \(M_{a,b}^{s}\) - p-ństwo przejścia z \(a\) do \(b\) w \(s\) krokach.
        \item \(f_{a,b}(s)\) - p-ństwo \textbf{pierwszego} przejścia z \(a\) do \(b\) w \(s\) krokach.
        \item \(f_{a,b} = \sum_{s=1}^{\infty} f_{a,b}(s)\). - p-ństwo przejścia z \(a\) do \(b\).
        \item \(p_{a,b}(s)\) - p-ństwo przejścia z \(a\) do \(b\) w \(s\) krokach.
        \item \textbf{Stan powracający} - \(f_{a,a} = 1\). \textbf{Stan chwilowy} - nie powracający.
        \item \(a\) jest powracający \(\iff \sum_{n=1}^{\infty} p_{a,a}(n) = +\infty\).
        \item Stan \(b\) jest osiągalny z \(a\) \(\iff p_{a,b} > 0\). \(b \in A(a)\).
        \item Stany \(a\) i \(b\) się komunikują \(\iff a \in A(b) \land b \in A(a)\).
        \item W skończonym łańcuchu Markowa dla każdego stanu \(a\) istnieje stan powracający \(b\) osiągalny z \(a\).
        \item Klasa to spójna silna składowa. Każda klasa składa się z samych stanów powracających lub chwilowych.
        \item Łańcuch \textbf{nieredukowalny} składa się z jednej klasy powracającej.
        \item Stan \(a\) jest okresowy, jeśli istnieje okres \(d > 1\), że jeśli \(p_{a,a}(k) > 0\)
         dla pewnego \(k\), to \(d | k\).
        \item Okresem stanu a nazywamy liczbę \(d\) (o ile istnieje) zdefiniowaną tak: \(d = NWD(\{k | p_{a,a}(k) > 0\})\).
        \item Jeżeli dwa stany Markowa się komunikują i jeden z nich jest okresowy to drugi też (z tym samym okresem).
        \item Nieredukowalny łańcuch z stanem o okresie \(d\) jest okresowy z okresem \(d\).
        \item \textbf{Twierdzenie Ergodyczne}: Jeśli łańcuch jest nieokresowy i nieredukowalny, to istnieje wektor \(\pi\), że:
        \begin{enumerate}[itemsep=0pt, left=0pt, topsep=0pt]
            \item suma współrzędnych = 1,
            \item \(\pi = \pi P\),
            \item \(\lim_{n \to \infty} p_{a,b}(n) = \pi_b\).
        \end{enumerate}  
        Wektor spełniający 1 i 2 to \textbf{rozkład stacjonarny}, a 1,2,3 \textbf{graniczny} łańcucha. Dla granicznego zachodzi \(\mu_{a,a} = \frac{1}{\pi_a}\)
    \end{itemize}

    \subsection*{Rozkład jednostajny \(Unif([a, b]) \)}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item Gęstość: \( f(z) = \frac{1}{b-a} \) dla \( z \in [a, b] \), 0 wpp.
        \item Dystrybuanta: \( F(z) = \frac{z-a}{b-a} \) dla \( z \in [a, b] \), 0 wpp.
        \item \(\mathbb{E}[X] = \frac{a+b}{2}, \quad \text{Var}(X) = \frac{(b-a)^2}{12}\).
        \item \textbf{Suma n-zmiennych jednostajnych \(Unif(0,1)\)}: \(X_1, X_2, \ldots, X_n \\
        \text{ niezależne o tym samym rozkładzie:} \\
         f_{X_1 + X_2 + \ldots + X_n}(z) =  \frac{t^{n-1}}{(n-1)!}\).
    \end{itemize}

    \subsection*{Rozkład wykładniczy \( X \sim \text{Exp}(\theta) \)}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item Dystrybuanta: \( F(z) = 1 - e^{-\theta z} \) dla \( z \geq 0 \), 0 wpp.
        \item Brak pamięci: \( P(X > s+t \mid X > s) = P(X > t) \). 
          
        \item \textbf{Suma n-zmiennych wykładniczych}: \(X_1, X_2, \ldots, X_n \\
        \text{ niezależne o tym samym rozkładzie:} \\
         f_{X_1 + X_2 + \ldots + X_n}(z) =  \frac{\theta^n}{(n-1)!}z^{n-1}e^{-\theta z}\).
    \end{itemize}

    \subsection*{Rozkład normalny/Gaussa \( X \sim \mathcal{N}(\mu, \sigma^2) \)}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item Dystrybuanta: \(\Phi(z)=F(z) = \int_{-\infty}^{z} f(t) \dd{t}\).
        \item Jeśli \(X \sim \mathcal{N}(\mu, \sigma^2)\), to \(cX+a \sim \mathcal{N}(c\mu+a, c^2\sigma^2)\).
        \item Suma: Jeśli \( X \sim \mathcal{N}(\mu_1, \sigma_1^2) \) i \( Y \sim \mathcal{N}(\mu_2, \sigma_2^2) \) są niezależne, to \( X+Y \sim \mathcal{N}(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2) \).
    \end{itemize}

    \subsection*{Własności rozkładów}

    \begin{itemize}[itemsep=0pt, left=0pt]
        \item \(EX = \int_{-\infty}^{\infty}{tf_X(t) \dd{t}}\), o ile funckja \(tf_X(t)\) jest całkowalna z modułem na \(\mathbb{R}\).
        \item \(E[g(X)] = \int_{-\infty}^{\infty}{g(t)f_X(t) \dd{t}}\), o ile funkcja \(g(t)f_X(t)\) jest całkowalna z modułem na \(\mathbb{R}\).
        \item \textbf{Twierdzenie DeMoivre'a-Laplace'a}: Niech \(S_n\) będzie liczbą sukcesów w n niezależnych próbach z
        prawdopodobieństwem sukcesu \(p\). Wówczas, dla każdego \(a < b\): \(P(a \leq \frac{S_n - np}{\sqrt{np(1-p)}} \leq b) \to \Phi(b) - \Phi(a)\)
        \item \textbf{Niezależność zmiennych losowych}: \(X,Y \text{ niezależne} \iff F_{X,Y}(x,y) = F_X(x)F_Y(y)\)
        \item \textbf{Suma niezależnych zmiennych losowych}: \(X,Y \text{ niezależne} \implies f_{X+Y}(z) = \int_{-\infty}^{\infty}{f_X(t)f_Y(z-t) \dd{t}}\)
        \item \textbf{p-ństwo całkowite}: \(P(X \in A) = \int_{-\infty}^{\infty}{P(X \in A | Y = y) f_Y(y) \dd{t}}\)
    \end{itemize}

    \subsection*{Niezależność zdarzeń}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item Rodzina zdarzeń niezależnych, każde o \( P(A) < 1 \), nie pokrywa całej przestrzeni zdarzeń.
        \item Jeśli \( X, Y \) są niezależne, \( f, g \) dowolne funkcje, to \( f(X) \) i \( g(Y) \) są niezależne.
    \end{itemize}
    
    \subsection*{Rozkład dwumianowy (\( X \sim \text{Binom}(n, p) \))}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item \( P(X=k) \) rośnie dla \( k \leq \lfloor (n+1)p \rfloor \), potem maleje.
        \item Granica: Jeśli \( n \to \infty, p \to 0, np = \lambda \), to \( \text{Binom}(n, p) \to \text{Pois}(\lambda) \).
        \item Suma: \( \text{Binom}(n_1, p) + \text{Binom}(n_2, p) \sim \text{Binom}(n_1+n_2, p) \).
    \end{itemize}
    
    \subsection*{Rozkład Poissona (\( X \sim \text{Pois}(\lambda) \))}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item \( P(X=k) \) rośnie dla \( k \leq \lfloor \lambda \rfloor \), potem maleje.
        \item Suma: \( \text{Pois}(\lambda_1) + \text{Pois}(\lambda_2) \sim \text{Pois}(\lambda_1+\lambda_2) \).
        \item Warunkowe: Dla \( X \sim \text{Pois}(\lambda_1), Y \sim \text{Pois}(\lambda_2) \): \\
        \(
        P(X=k \mid X+Y=n) = \binom{n}{k} \left(\frac{\lambda_1}{\lambda_1+\lambda_2}\right)^k \left(\frac{\lambda_2}{\lambda_1+\lambda_2}\right)^{n-k}.
        \)
    \end{itemize}
    
    \subsection*{Rozkład geometryczny (\( X \sim \text{Geom}(p) \))}
    \begin{itemize}[itemsep=0pt, left=0pt]
        \item Brak pamięci: \( P(X=n \mid X>m) = P(X=n-m) \).
        \item Własność definiująca: Zmienna \( X \) przyjmująca wartości 1, 2 \(\ldots\)  o brakującej pamięci ma rozkład geometryczny.
        \item Minimum: Jeśli \( X \sim \text{Geom}(p), Y \sim \text{Geom}(q) \ \text{niezależne} \), to \( \min(X, Y) \sim \text{Geom}(1-(1-p)(1-q)) \).
    \end{itemize}
    
    \section*{Inne własności}
    \begin{itemize}
    
    \item \textbf{Wartość oczekiwana z funkcji generującej momenty}: \(\mathbb{E}[X] = M_X'(0)\), gdzie \( M_X(t) = \mathbb{E}[e^{tX}] \).
    
    \item \textbf{Wartość oczekiwana z funkcji tworzącej prawdopodobieństwa} \( g_X(t) = \mathbb{E}[t^X] \): \(\mathbb{E}[X] = \lim_{t \to 1^-} g_X'(t)\), 
    a dla funkcji \( g_X(t) \) zbieżnych na całej prostej rzeczywistej: \(\mathbb{E}[X] = g_X'(1)\)
    
    \item \textbf{Wariancja z funkcji tworzącej prawdopodobieństwa}: 
    Jeśli \( X \) ma skończoną wartość oczekiwaną i wariancję, to:
    \(
    \mathbb{E}[X^2] = \lim_{t \to 1^-} \big(g_X'(t) + g_X''(t)\big),
    \)
    a wariancja wynosi:
    \(
    \mathrm{Var}(X) = \lim_{t \to 1^-} \big(g_X'(t) + g_X''(t) - (g_X'(t))^2\big).
    \)
    
    \item \textbf{Wartość oczekiwana i wariancja z funkcji generującej prawdopodobieństwa}: 
    Jeśli \( g_X(t) \) oraz \( g_X'(t) \) są zbieżne na całej prostej rzeczywistej, to:
    \(
    \mathbb{E}[X^2] = g_X'(1) + g_X''(1),
    \), a wariancja wynosi:
    \(
    \text{Var}(X) = g_X'(1) + g_X''(1) - (g_X'(1))^2.
    \)
    
    \end{itemize}




\end{multicols}
\end{document}